//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_75
.address_size 64

	// .globl	test_mod_add
.const .align 8 .b8 _P[32] = {47, 252, 255, 255, 254, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255};
.const .align 8 .b8 _Gx[32] = {152, 23, 248, 22, 91, 129, 242, 89, 217, 40, 206, 45, 219, 252, 155, 2, 7, 11, 135, 206, 149, 98, 160, 85, 172, 187, 220, 249, 126, 102, 190, 121};
.const .align 8 .b8 _Gy[32] = {184, 212, 16, 251, 143, 208, 71, 156, 25, 84, 133, 166, 72, 180, 23, 253, 168, 8, 17, 14, 252, 251, 164, 93, 101, 196, 163, 38, 119, 218, 58, 72};

.visible .entry test_mod_add(
	.param .u64 test_mod_add_param_0,
	.param .u64 test_mod_add_param_1,
	.param .u64 test_mod_add_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd19, [test_mod_add_param_0];
	ld.param.u64 	%rd20, [test_mod_add_param_1];
	ld.param.u64 	%rd18, [test_mod_add_param_2];
	cvta.to.global.u64 	%rd21, %rd20;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	shl.b32 	%r5, %r4, 2;
	cvta.to.global.u64 	%rd22, %rd19;
	mul.wide.s32 	%rd23, %r5, 8;
	add.s64 	%rd24, %rd22, %rd23;
	add.s64 	%rd25, %rd21, %rd23;
	ld.global.u64 	%rd26, [%rd25];
	ld.global.u64 	%rd27, [%rd24];
	add.s64 	%rd1, %rd27, %rd26;
	setp.lt.u64 	%p1, %rd1, %rd27;
	selp.u64 	%rd28, 1, 0, %p1;
	ld.global.u64 	%rd29, [%rd24+8];
	add.s64 	%rd30, %rd29, %rd28;
	setp.lt.u64 	%p2, %rd30, %rd29;
	selp.u64 	%rd31, 1, 0, %p2;
	ld.global.u64 	%rd32, [%rd25+8];
	add.s64 	%rd2, %rd30, %rd32;
	setp.lt.u64 	%p3, %rd2, %rd30;
	selp.u64 	%rd33, 1, 0, %p3;
	add.s64 	%rd34, %rd33, %rd31;
	ld.global.u64 	%rd35, [%rd24+16];
	add.s64 	%rd36, %rd35, %rd34;
	setp.lt.u64 	%p4, %rd36, %rd35;
	selp.u64 	%rd37, 1, 0, %p4;
	ld.global.u64 	%rd38, [%rd25+16];
	add.s64 	%rd3, %rd36, %rd38;
	setp.lt.u64 	%p5, %rd3, %rd36;
	selp.u64 	%rd39, 1, 0, %p5;
	add.s64 	%rd40, %rd39, %rd37;
	ld.global.u64 	%rd41, [%rd24+24];
	add.s64 	%rd42, %rd41, %rd40;
	setp.lt.u64 	%p6, %rd42, %rd41;
	ld.global.u64 	%rd43, [%rd25+24];
	add.s64 	%rd61, %rd42, %rd43;
	setp.lt.u64 	%p7, %rd61, %rd42;
	selp.u64 	%rd44, 1, 0, %p7;
	selp.b64 	%rd45, -1, 0, %p6;
	setp.ne.s64 	%p8, %rd44, %rd45;
	ld.const.u64 	%rd4, [_P+24];
	setp.gt.u64 	%p9, %rd61, %rd4;
	or.pred  	%p10, %p8, %p9;
	@%p10 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_1;

$L__BB0_7:
	ld.const.u64 	%rd60, [_P+16];
	bra.uni 	$L__BB0_8;

$L__BB0_1:
	setp.lt.u64 	%p11, %rd61, %rd4;
	mov.u64 	%rd62, %rd3;
	mov.u64 	%rd63, %rd2;
	mov.u64 	%rd64, %rd1;
	@%p11 bra 	$L__BB0_9;

	ld.const.u64 	%rd60, [_P+16];
	setp.gt.u64 	%p12, %rd3, %rd60;
	@%p12 bra 	$L__BB0_8;

	setp.lt.u64 	%p13, %rd3, %rd60;
	mov.u64 	%rd62, %rd3;
	mov.u64 	%rd63, %rd2;
	mov.u64 	%rd64, %rd1;
	@%p13 bra 	$L__BB0_9;

	ld.const.u64 	%rd7, [_P+8];
	setp.gt.u64 	%p14, %rd2, %rd7;
	@%p14 bra 	$L__BB0_8;

	setp.lt.u64 	%p15, %rd2, %rd7;
	mov.u64 	%rd62, %rd3;
	mov.u64 	%rd63, %rd2;
	mov.u64 	%rd64, %rd1;
	@%p15 bra 	$L__BB0_9;

	ld.const.u64 	%rd46, [_P];
	setp.lt.u64 	%p16, %rd1, %rd46;
	mov.u64 	%rd62, %rd3;
	mov.u64 	%rd63, %rd2;
	mov.u64 	%rd64, %rd1;
	@%p16 bra 	$L__BB0_9;

$L__BB0_8:
	ld.const.u64 	%rd47, [_P];
	sub.s64 	%rd64, %rd1, %rd47;
	setp.lt.u64 	%p17, %rd1, %rd47;
	selp.u64 	%rd48, 1, 0, %p17;
	sub.s64 	%rd49, %rd2, %rd48;
	ld.const.u64 	%rd50, [_P+8];
	sub.s64 	%rd63, %rd49, %rd50;
	add.s64 	%rd51, %rd50, %rd48;
	setp.lt.u64 	%p18, %rd2, %rd51;
	selp.u64 	%rd52, 1, 0, %p18;
	sub.s64 	%rd53, %rd3, %rd52;
	sub.s64 	%rd62, %rd53, %rd60;
	add.s64 	%rd54, %rd60, %rd52;
	setp.lt.u64 	%p19, %rd3, %rd54;
	selp.u64 	%rd55, 1, 0, %p19;
	sub.s64 	%rd56, %rd61, %rd55;
	sub.s64 	%rd61, %rd56, %rd4;

$L__BB0_9:
	cvta.to.global.u64 	%rd57, %rd18;
	add.s64 	%rd59, %rd57, %rd23;
	st.global.u64 	[%rd59], %rd64;
	st.global.u64 	[%rd59+8], %rd63;
	st.global.u64 	[%rd59+16], %rd62;
	st.global.u64 	[%rd59+24], %rd61;
	ret;

}

